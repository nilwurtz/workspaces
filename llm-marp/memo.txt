## LLMはテキスト補完エンジンにすぎない

*   **テキスト補完エンジンである**
    *   LLMは、**トレーニング中に提供されるテキストを模倣する**ように学習します。与えられたプロンプト（文字列の冒頭部分）に対して、そのドキュメントが自然に続くと思われるテキストを生成（補完）します。
    *   **具体例**: 「One, Two,」というプロンプトに対して、LLMは「Buckle My Shoe」と補完します。また、テレビが壊れた状況の続きを尋ねるプロンプトに対して、LLMが短編小説を学習していれば「本を読むことにしました」と答え、会話ログを学習していれば「壁のコンセントから抜いて再度差し込んでみてください」とアドバイスのような回答を返すなど、トレーニングデータによって異なる補完をします。

*   **自己回帰モデルであり、1トークンずつ生成し、途中で修正や戻り作業ができない**
    *   LLMは1回処理するごとに統計的に最も可能性の高い「次のトークン」を予測・生成し、一度出したトークンは後戻りして修正できません。
    *   **具体例**: ChatGPTに単語内の文字を逆転させようとすると、トークンを分解したり再組み立てしたりする文法的な処理が非常に難しいため、正しく行えません。また、プロンプトの後に文字数カウントのリクエストが来ても、LLMはテキストを一度しか読まず、読み直して数えることができないため、正確な文字数を答えることができません（例えば、正解442文字に対し823文字と回答する例がある）。

*   **パターン認識に長ける一方で、生成したパターンにはまり込み、過度な繰り返し出力が発生することがある**
    *   LLMはパターン認識が得意なため、何らかのパターンが生じると、どこで打ち切るべきか見つけられなくなることがあります。
    *   **具体例**: OpenAIのtext-curie-001モデルが生成した「あるテレビ番組を好む理由」のリストが、項目が延々と繰り返されたり、「遺産」「未来」「情熱的」といった特定の単語が延々と繰り返されたりするなど、極端に反復的な出力になることがあります。

*   **テキストを文字単位ではなく「トークン」単位で認識し処理する**
    *   LLMは文字列を文字の並びではなく、通常3～4文字程度の「トークン」と呼ばれる複数文字のチャンクとして認識し、人間のように一文字ずつゆっくり確認することはできません。
    *   **具体例**: 「strawberryに含まれるRの数は？」といった単純な文字カウントの質問にも簡単に惑わされることがあります。また、「ghost」が1つのトークンであるのに対し、誤字の「gohst」は「g」「oh」「st」の3つのトークンに分解されるなど、綴り間違いをまったく別のものとして扱います。さらに、大文字変換を苦手とすることもあります。

*   **ハルシネーション（幻覚）を生成する傾向がある**
    *   LLMは事実と異なるがもっともらしく見える情報を自信を持って生成してしまうことがあります。
    *   **具体例**: プロンプトが実在しないものについて言及すると、LLMは通常、それが本当に存在すると想定し続ける傾向があります。これは「真実バイアス」と呼ばれ、LLMは与えられたプロンプトを正しいものとみなして振る舞おうとし、誤りを正す可能性はごく低いとされています。


### LLMの生成プロセスの特徴と人間との比較



#### 自己回帰性（オートリグレッシブモデル）

LLMは、「複数のトークン」から「次の1つのトークン」を予測し続け、この操作を必要なだけ繰り返して単一トークンを積み重ね、最終的にテキストを生み出します。予測されたトークンはプロンプトの末尾に付け足され、新たなプロンプトとして次のトークンの予測に用いられます。





#### 思考時間（内省）の欠如と即時性

人間は文章作成中に立ち止まり、考え、振り返ることができます。また、質問に答える前に、自身の知識を検索したり、内省したりする別のモードを持っています。



LLMは、人間とは異なり**「もう少し考える時間が欲しい」という猶予は与えられず、立ち止まることもできません**。LLMには内的な独り言のような機能が存在しないため、回答を出す前に問題について実際に「考える」ことができません。その代わりに、各トークンは先行するすべてのトークンの関数として機械的に生成されます。



#### 後戻り（修正）の不可逆性

人間はテキスト作成中に間違いを見つければ後戻りして修正することができます。

LLMは、一度トークンを出してしまうと、そのトークンを覆すことができません。後戻りして消去できないのです。LLMは、人間が執筆中に誤字脱字を明示的に取り消したような「完成前のドキュメント」でトレーニングされていないため、以前の出力が誤りであることを後から指摘して取り消そうともしません。この特性ゆえに、LLMは筋の通らない方向へと突き進み、結果として「頑固で滑稽に見えてしまう」ことがあります。





#### 模倣とパターン認識能力

LLMは、トレーニングデータに見られるパターン、特に論理的な推論や展開の仕方を抽象化し、新たなプロンプトに対しても柔軟に補完することを目指します。



パターン認識に優れているため、生成されたテキストに何らかのパターンが生じると、どこで打ち切るべきか見つけられなくなり、極端に繰り返しの多い出力が発生することがあります。





#### ハルシネーション（幻覚）と真実バイアス

LLMは、事実と異なるがもっともらしく見える情報を自信を持って生成してしまう「ハルシネーション」という残念な副作用があります。モデルの視点からは他の補完と区別がつかないため、「勝手に作り話をしないで」といった指示はほとんど効果がありません。



プロンプトに提示された情報を正しいものとみなす「真実バイアス」も持っています。そのため、プロンプトが実在しないものに言及しても、LLMはそれが本当に存在すると仮定し続けてしまいます。



#### トークン化とその制約



LLMはテキストを文字の並びとして直接見るのではなく、「トークン」と呼ばれる複数文字のチャンクに分解して処理します。



人間は単語の境界を曖昧に処理しますが、LLMは決定論的なトークナイザーを使用するため、綴り間違いが目立ちます。



LLMは文字を1つずつゆっくり確認できず、トークンを分解したり再組み立てしたりする文法的な処理が非常に困難です。



大文字と小文字の区別も、人間とは異なり、LLMが使うトークンではまったく異なるものとして扱われることがあります。





#### コンテキストウィンドウサイズ（トークン制限）



LLMは、好きなだけ長いテキストを受け取り、好きなだけ長いテキストを出せるわけではなく、「コンテキストウィンドウサイズ」という固定されたトークン数の制約があります。これを埋め尽くそうとすると、情報過多でモデルが混乱したり、無関係な補完を生成したりする可能性があります。





#### 速度と計算コスト



LLMは長いプロンプトを読み込むのは得意ですが、長い補完テキストを生成するのはずっと遅くなります。プロンプト部分の処理は生成部分よりもおよそ1桁速いとされています。



大きなモデルほど出力品質は向上しますが、コストや待ち時間が増加するというトレードオフがあります。





#### 指示に従う能力（アラインメント）



人間のフィードバックによる強化学習（RLHF）によって、「有用・正直・無害」なアシスタントとしての役割を身につけ、ユーザーの指示に従うようにトレーニングされています。



ただし、RLHFプロセスによってモデルの知能が失われる場合があり、これを「アラインメント税」と呼びます.

---

LLMアプリケーションを利用する側として、「アラインメント税（alignment tax）」は、モデルを「有用・正直・無害（helpful, honest, and harmless、HHH）」なアシスタントにするための**人間のフィードバックによる強化学習（RLHF）プロセスが、皮肉にもモデルの知能を損なう場合がある**という問題です。

これは、RLHFが追求する「有用・正直・無害」という3つの基準が、**必ずしもモデルの知能そのものとは一致しない**ため、トレーニング中に**ある自然言語処理タスクでモデルがかえって性能を落とす**ことがあります。モデルがバーチャルアシスタントという「特定の」役割に特化することで、**他のタスクでの品質が低下する**可能性が生じます。

具体的な実例としては、以下の問題が挙げられています。

*   **GPT-4の能力低下の報告**：
    2023年7月のスタンフォード大学の論文「How Is ChatGPT's Behavior Changing Over Time」（ChatGPTの振る舞いは時間とともにどのように変化しているか）によれば、**GPT-4がある領域で徐々に能力が低下している**と報告されています。これは、特定のタスクや振る舞いにモデルをファインチューニングする際に、パフォーマンス低下に気を付ける必要があるという警告でもあります。
*   **人間らしい多様性の喪失**：
    RLHFでファインチューニングされたチャットモデルには、**人間の多様性が失われる傾向がある**という大きな課題があります。これらのモデルは**意図的に礼儀正しく均質化されている**ため、インターネット上で見られる人間らしい多様な振る舞い（時に無礼だったり、不快だったり、偏見のあるものを含む）が希薄化してしまっているのです。これは、以下のような特定の用途で問題を引き起こします。
    *   **自然言語のサンプルデータ生成**: 他のプロジェクトで使用する自然言語のサンプルデータを生成する際、礼儀正しいアシスタントを通したフィルタリングは不要であり、**かえって多様なデータが得られない**という問題が生じます。
    *   **医師によるブレインストーミング**: 医師が患者の治療選択肢を検討するためにブレインストーミングを行う際に、モデルが「**専門家に相談してください**」といった**説教じみた回答を返すことがあり、これは時間の無駄**になってしまいます。
    *   **警察の業務**: 警察がモデルと対話する際に、「**違法行為については話せません**」というような**一律の拒否は業務の妨げ**となります。

しかし、これらの問題を軽減する手法も存在し、全体的に見ればモデルの能力は着実に向上し続けているとされています。OpenAIは、ベースモデル用の元トレーニングデータの一部を混ぜ合わせて学習させることで、このアラインメント税を抑えつつ、モデルが本来の能力を維持しながら「有用・正直・無害」へと最適化されることを確認しています。


----
「チェーホフの銃の誤謬（Chekhov's gun fallacy）」は、LLMが**トレーニングデータで見られるパターンを模倣し、与えられたすべての情報に意味を見出そうとする**という特性に紐づいています。

具体的には、LLMが「テキスト補完エンジンに過ぎない」という本質から、以下の落とし穴につながります。

*   **無関係な情報への意味付け**:
    *   劇作家アントン・チェーホフの提唱する原則では、「第一幕で壁に銃を掛けるのであれば、次の幕でそれを撃たなければならない。もしそうでないのなら、最初からそこに掛けるべきではない」と述べられています。これは、物語に登場する要素はすべて必要不可欠であるべきだという考え方です。
    *   人間は意識的・無意識的にこの原則に従う傾向があり、**LLMもトレーニングデータを通じてこの考え方を学習しています**。
    *   そのため、LLMは、**たとえプロンプトの中に無関係なコンテキストが含まれていても、それを重要な情報であると解釈しようとし、深読みしてしまう**傾向があります。これが「チェーホフの銃の誤謬」の本質であり、モデルが誤った方向に導かれる原因となる可能性があります。

*   **真実バイアス**:
    *   この誤謬は、LLMがプロンプトに提示された情報を正しいものとみなす傾向がある「**真実バイアス**」とも関連しています。プロンプトが実在しないものに言及しても、LLMはそれが本当に存在すると仮定し続けてしまいます。プログラムでプロンプトを生成する際に、誤った情報や非論理的な要素が混入すると、LLMはそれを正すことなく、あたかも正しいかのように振る舞おうとします。

この落とし穴を避けるためには、生成結果に関連する**適切なスニペット（情報の断片）のみをプロンプトに含める**ことが唯一確実な方法だとされています。また、プロンプトに不要な情報が多すぎると、モデルが混乱したり、プロンプトのコンテキストウィンドウを無駄に消費したり、処理が遅くなったり、コストが上がったりするリスクもあります。