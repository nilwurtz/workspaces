---
marp: true
theme: default
paginate: true
size: 16:9
style: |
  @import url("https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@200;400;700&family=Noto+Serif+JP:wght@200;400;700&display=swap");
  * {
    font-family: "Noto Sans JP", sans-serif;
  }

  section {
    font-size: 26px;
    line-height: 1.4;
    background-color: #f6f6f6;
  }
  h1 {
    color: #7F55B1;
    font-size: 44px;
  }
  h2 {
    color: #9B7EBD;
    font-size: 32px;
  }
  h3 {
    color: #F49BAB;
    font-size: 28px;
  }
  .highlight {
    background-color: #FFE1E0;
    padding: 15px;
    border-radius: 8px;
    margin: 15px 0;
    font-size: 24px;
  }
  .problem {
    background-color: #FFE1E0;
    padding: 12px;
    border-left: 4px solid #F49BAB;
    margin: 12px 0;
    font-size: 22px;
  }
  .solution {
    background-color: #FFE1E0;
    padding: 12px;
    border-left: 4px solid #9B7EBD;
    margin: 12px 0;
    font-size: 22px;
  }
  .code-small {
    font-size: 18px;
  }
  pre {
    font-size: 20px;
  }
  .engineer-focus {
    background-color: #FFE1E0;
    padding: 15px;
    border-left: 4px solid #7F55B1;
    margin: 15px 0;
  }
  .ref {
    font-size: .6em;
    color: #666;
    display: block;
    text-align: center;
    margin-top: .3em;
  }
  .center-image {
    text-align: center;
  }
  .center-image img {
    display: block;
    margin: 0 auto;
  }
---

# なぜLLMアプリケーションへの指示は失敗するのか？

## ソフトウェアエンジニアが知るべきLLMの特性

<div class="highlight">
📊 目標：LLMを理解して「これはうまくいきそう感」を得る<br>
🔍 目標：失敗の原因をプロファイルしやすくする
</div>

---

## tl;dr

**読もう**

<div class="center-image">

![w:400](images/40113_prompt_engineering_for_llms_cvr.jpg)

</div>


---

## 今日のアジェンダ

1. **LLMの基本** - テキスト補完エンジンの実態
2. **LLMの制約** - なぜ期待通りに動かないのか
3. **失敗パターン** - エンジニアがハマりがちな罠
4. **プロンプトエンジニアリング** - 実践的な改善手法
5. **LLMアプリケーション** - AIエージェントの仕組み

---

# 1. LLMの基本

## 知能はない。考えてもいない。テキストを補完しているだけ。

---

## LLMとは何か？

### 大規模言語モデル（Large Language Model）

- **本質**: 次の単語を予測するタスクを繰り返す **テキスト補完エンジン**
- **学習**: 大量のテキストデータから言語のパターンや構造を学習

![w:600 h:300 cover](/images/04-gpt3-generate-tokens-output.gif)

<span class="ref">ref: https://jalammar.github.io/how-gpt3-works-visualizations-animations/
</span>

---

**次の単語に何を選ぶと一番それっぽいでしょう？クイズ**をひたすらに答え続けている
```
["Shohei", "Ohtani", "is", "a"] -> "baseball"? "pitcher"? "great"?

↓ "baseball"

["Shohei", "Ohtani", "is", "a", "baseball"] -> "player"? "pitcher"? "star"?

↓ "player"

["Shohei", "Ohtani", "is", "a", "baseball", "player"] -> "who"? "that"? "from"?
```


---

### 実際には「次のトークン」を予測している
- **例**
"strange new worlds" -> [str][ange][ new][ worlds]
特に単語というわけではない。

- **大文字・小文字で全く扱いが変わる**
"STRANGE NEW WORLDS" -> [STR][ANGE][ NEW][ WOR][L][DS]


---

## Q: LLMはなぜ「Chat」できるのか？

---



### LLMはなぜ「Chat」できるのか？

A.対話データを学習しているから

```yaml
# 学習データに含まれる対話形式の例 (これはちょっと古いらしい)
system: "あなたは親切なプログラミングアシスタントです"
user: "Pythonでリストをソートする方法は？"
assistant: "Pythonでリストをソートする方法をいくつか紹介します..."
user: "逆順にソートするには？"
assistant: "逆順にソートするには..."
```

<div class="engineer-focus">
💻 LLMは「アシスタントらしい応答」を学習しているだけ<br>
実際に理解しているわけではなく、パターンを模倣している
</div>


---

### LLMの応用性の高さ

LLMの基礎は「テキスト補完エンジン」だが、
学習データの中身によって対話・質問応答・コード生成など多様なタスクに対応可能


→ この柔軟性と自由度により、Tool呼び出し・タスク実行型AI Agentが実現


---

# 2. LLMの制約

## なぜ期待通りに動かないのか？

---

## 制約1: 自己回帰モデル

### 一方向性：未来を見ることができない

#### 自己回帰モデルの制約

**LLMは「左から右へ」の一方向処理のみ可能**

- **後方参照のみ**: 過去のトークンしか参照できない
- **未来予測不可**: 後続の内容を事前に知ることはできない
- **層間制約**: 上位層の情報を下位層に戻せない

<div class="engineer-focus">
💻 **アーキテクチャ的制約**<br>
・Transformer の Decoder-only 構造<br>
・Masked Self-Attention により未来のトークンをマスク<br>
・各レイヤーは前段の出力のみを入力として受け取る
</div>

**この制約により「読み返し」「見直し」が根本的に不可能**

---

TODO: LLMの図を入れる。



---

## 制約の具体的な影響

<div class="problem">
❌ **文字数カウント**: "strawberryに含まれるRの数は？"<br>
→ すでに処理済みの文字列を「遡って」解析できない
</div>

<div class="problem">
❌ **文字列逆転**: "hello を逆順にして"<br>
→ トークン化＋一方向性により困難
</div>

<div class="solution">
✅ **前向き処理**: "これから書く文章の文字数を数えながら書いて"<br>
→ 生成しながら同時にカウント可能（ただし精度は低い）
</div>

<div class="engineer-focus">
💻 <strong>エンジニアへの示唆</strong>: 正規表現的な厳密な文字列処理、後方参照が必要な処理はLLMの苦手分野
</div>

---

## 制約2: トークンベースの処理 に起因する認識の限界

### トークン化による文字レベル情報の損失

```python
# トークン化の例（GPT系モデル）
"ghost"     → ["ghost"]           # 1トークン
"gohst"     → ["g", "oh", "st"]   # 3トークン（別物として認識）
"GitHub"    → ["Git", "Hub"]      # 2トークン
"github"    → ["github"]          # 1トークン（大小文字で異なる）
```

#### 一方向処理との組み合わせ効果

<div class="problem">
❌ **誤字検出の困難さ**<br>
"gohst"を見たとき、一方向処理により"ghost"との関連性を認識しにくい<br>
文脈から推測するしかない
</div>

<div class="engineer-focus">
💻 **実用的影響**: 変数名の typo、API名の微細な違い、ファイルパスの大小文字違いなどを正確に扱えない
</div>

---


![](/images/08-gpt3-tokens-transformer-blocks.gif)

ref: https://jalammar.github.io/how-gpt3-works-visualizations-animations/


---

### トークン化の実用的な問題

<div class="problem">
❌ **厳密な文字列処理**<br>
・正規表現パターンの生成・検証<br>
・ASCIIアート、図表の生成<br>
・コードフォーマットの行数指定<br>
・変数名のスペルチェック
</div>

<div class="solution">
✅ **トークンレベルで処理しやすいタスク**<br>
・自然言語の生成・要約<br>
・意味的類似性の判断<br>
・構造化されたコード生成<br>
・API仕様の理解・説明
</div>

---

## 制約3: 後戻りできない・修正不可

### 一度生成された情報の修正・取消しの困難さ

<div class="problem">
❌ **典型的な問題**: 間違った前提で回答開始<br>
"このAPIの仕様は..." → 誤った情報を出力開始<br>
→ 途中で間違いに気づいても修正せず継続<br>
→ 一貫して間違った結論に到達
</div>

基本的には「だめだこの感じ」となった時点で割り込んであげるのが吉。

---

## 制約4: パターンに固執、繰り返し発生

### 1. 統計的パターンからの脱却困難

パターンを見出すのが得意なLLMは、学習データに基づいて「よくあるパターン」を繰り返す傾向がある。
最新のモデルだとあまり起きない。

<div class="problem">
❌ <strong>無限ループ的出力</strong><br>
・リスト生成時の同じ項目の反復<br>
・コード生成時の同じパターンの繰り返し<br>
・「遺産」「未来」「情熱的」といった特定語彙の連続出力
</div>



---

## 制約5: 情報順序への依存性

### 「Lost in the Middle」現象

<div class="problem">
❌ <strong>プロンプト順序の影響</strong><br>
・冒頭と末尾の情報は記憶されやすい<br>
・中間部分の情報は活用されにくい<br>
・指示の位置により出力品質が大きく変動
</div>

**具体例**: 段落の文字数カウント依頼を段落の後に置くと、LLMは段落処理時にカウント意図を知らないため失敗

---

## 制約6: 思考時間の欠如

### 内省的思考プロセスの不在

<div class="problem">
❌ <strong>即時応答の制約</strong><br>
・「考える時間」が存在しない<br>
・内的な "独り言" による検討ができない<br>
・複雑な推論の段階的構築が困難
</div>

<div class="solution">
✅ <strong>Chain of Thought (CoT) による対策</strong><br>
・思考プロセスの外部化を強制<br>
・「ステップバイステップで考えて」指示<br>
・中間推論の明示的出力により品質向上
</div>

<div class="engineer-focus">
💻 LLMに「考えさせる」には思考を出力として表現させる必要がある
</div>

---

## 制約5: ハルシネーションと真実バイアス

### プロンプトの情報を「正しい」と過信

<div class="problem">
❌ チェーホフの銃の誤謬<br>
→ 無関係な情報でも「重要だから含まれているはず」と解釈<br>
→ 実在しない情報でも「存在する」と仮定して回答
</div>

<div class="engineer-focus">
💻 TODO：実際の例
</div>

---

# 3. 失敗パターン

## エンジニアがハマりがちな罠

---

## 失敗パターン1: 曖昧なタスク定義

<div class="problem">
❌ 悪い例："このコードをリファクタリングして"<br>
❌ 悪い例："APIドキュメントを作成して"<br>
❌ 悪い例："バグを修正して"
</div>

<div class="solution">
✅ 良い例："このPython関数を、読みやすさを重視してリファクタリングしてください。変数名を明確にし、コメントを追加し、型ヒントを付けてください"
</div>

---

## 失敗パターン2: 複数タスクの同時依頼

<div class="problem">
❌ 悪い例：<br>
"このReactコンポーネントを修正して、テストも書いて、TypeScriptに変換して、ドキュメントも更新して"
</div>

<div class="solution">
✅ 良い例：段階的に実行<br>
1. まずReactコンポーネントを修正<br>
2. 修正内容を確認後、TypeScript変換<br>
3. 変換後にテスト作成<br>
4. 最後にドキュメント更新
</div>

---

## 失敗パターン3: 文脈情報の不足

<div class="problem">
❌ 悪い例："このエラーを解決して"<br>
（どのエラー？どの環境？何をしようとしていた？）
</div>

<div class="solution">
✅ 良い例：<br>
"Next.js 13のApp RouterでuseRouter()を使おうとしたところ、'useRouter' is not a function エラーが発生しました。pages/index.jsからapp/page.jsに移行中です。解決方法を教えてください"
</div>

---

# 4. プロンプトエンジニアリング

## 実践的な改善手法

---

## 手法1: 構造化プロンプト

```yaml
## 役割
あなたは経験豊富なPythonエンジニアです

## タスク
以下のコードの性能を改善してください

## 制約条件
- Python 3.9以上を使用
- 外部ライブラリは numpy, pandas のみ使用可
- メモリ使用量を重視

## 入力コード
[コードをここに]

## 期待する出力
- 改善されたコード
- 変更点の説明
- 性能向上の見積もり
```

---

## 手法2: Few-Shot Prompting（例示による学習）

TODO:以下の内容を２枚程度のスライドにいれる。

LLM (大規模言語モデル) は、プロンプト内のパターンを認識し、それに従って出力を生成する能力に長けています。この特性を効果的に活用するプロンプトエンジニアリングの主要な手法の一つに、**Few-shotプロンプティング**があります。

### Few-shotプロンプティングとは

Few-shotプロンプティングとは、LLMに与えるプロンプトの中に、**期待する出力の形式やスタイルを示す具体的な例（サンプル）をいくつか含める**手法です。これにより、モデルは単に質問を解釈する方法だけでなく、生成すべき補完の具体的な形式まで明確に理解し、模倣することができます。

この手法は、RLHF（人間のフィードバックによる強化学習）によって、より丁寧で有用なレスポンスをするようにトレーニングされたLLMに対して特に効果を発揮します。現代のLLMは、わずか数例（"a few shots"）からでも有用なパターンを抽出し、類似のタスクを実行できることが示されています。

一方で、例をまったく与えず、明示的な指示のみで構成されるプロンプトは**Zero-shotプロンプト**と呼ばれます。Few-shotプロンプトとZero-shotプロンプトの構造の違いは、ソースの図5-2で示されています。

### Few-shotプロンプティングの利点

*   **期待する形式・スタイルを明確に伝えられる**: LLMはパターンを踏襲する傾向が強いため、Q&Aのやり取りに特定のルールや形式がある場合、明示的な指示よりも具体例として示した方が効果的です。これにより、回答の微妙なニュアンスまでコントロールできます。例えば、書評の評価スタイル（辛口か寛容か）をサンプルを通じて指定し、モデルに一貫したキャラクター性を持たせることが可能です。
*   **指示の簡素化**: 質の高い例をいくつか用意できる場合、詳細なルールを文章で長々と列挙するよりも、Few-shotプロンプティングで例示する方が単純で扱いやすいことが多いです。これは、多くのルールが「見ればわかる」という性質を持つためです。
*   **モデルの理解を促進**: モデルにとってタスクの実行方法が明確でない場合に、具体的な参考例を示すことで非常に有効です。

### Few-shotプロンプティングの欠点と実例

Few-shotプロンプティングは強力な手法ですが、以下の3つの重要な欠点があります。

1.  **コンテキストが増えるほど扱いづらくなる**:
    *   問題解決に大量のコンテキスト情報を必要とする場合、例示が増えることでプロンプトが長くなり、LLMの「アテンション機構」が機能しにくくなる可能性があります。モデルが多様なソースから情報を得る際に、例示された情報と主要なタスクのバランスを取るのが難しくなります。
    *   過度に単純化された例を用いると、豊富なコンテキストによって可能となるはずの深い推論や繊細な判断をモデルが適切に行えなくなるリスクがあります。

2.  **例示された情報に偏る（アンカリング）**:
    *   LLMは、最初に与えられた不完全な情報（「ほんの一例」）に引きずられ、それが「普通」だと誤解し、その先入観がその後の判断に過度な影響を与えることがあります。これは「アンカリング」と呼ばれる認知バイアスに似ています。
    *   **実例**: ソースの図5-4では、女の子の赤ちゃんの名前が「最も人気があった年」を予測する例が示されています。プロンプトで「20世紀初頭」の年を例示すると、モデルもその範囲で予測しますが、「21世紀初頭」の年を例示すると、それに合わせて予測結果が大きく変化します。
    *   例示は、対象となるデータの確率分布の特性をモデルに伝達し、最終的な補完結果に影響を及ぼします。そのため、実際のデータ分布から大きく逸脱するような例示は避けるべきです。

3.  **誤ったパターンを示唆する可能性がある**:
    *   与えられた例の中に意図しないパターンが含まれていると、LLMがそれを拾い上げて、望ましくない形で反映してしまうことがあります。
    *   **実例**: ソースの図5-6では、女の子の赤ちゃんの名前とその人気があった年を提示する際に、年が「昇順」に並んでいるか「降順」に並んでいるかによって、モデルが次に生成する予測結果が異なることが示されています。プロンプトの例は決してランダムな順序にはならないため、意図せずパターンが生まれてしまう可能性が高まります。
    *   また、「良いケース（正常系）を先に、悪いケース（異常系）を後に」という一般的な順序付けパターンも、モデルに不当に悲観的な判断をさせる可能性があります。ソースの図5-7では、この順序付けがモデルの回答に与える影響が示されています。

### Few-shotプロンプティングの効果的な活用法

これらの欠点があるにもかかわらず、Few-shotプロンプティングは有用なツールです。
*   **コンテキスト増大への対処**: 出力形式など特定の側面のみを説明する場合は、小規模な例でも十分その意図を伝えることが可能です。
*   **アンカリングへの対処**: 極端に狭い期待値を伝えないよう、適切な範囲の例を提供するべきです。また、実際の過去の事例にアクセスできる場合は、それらの中から代表的なサンプルを抽出してFew-shotプロンプトに組み込むことで、より現実的な分布を実現できます。
*   **誤ったパターンへの対処**: 収集した例の中から一部を抽出してランダムに並び替え、最終結果への影響を評価する手法があります。近年では、DSPyのような「プロンプト最適化」ツールも登場しており、Few-shotの例を体系的に選択・並び替えを行い、精度などの評価指標を最適化する方法を提供しています。
*   **使用の判断**: モデルにとってタスクの実行方法が明確でない場合に非常に有効ですが、モデルがすでにタスクを理解できていると思われる場合は、プロンプトが冗長になるだけでなく、上記のような問題が発生するリスクも高まるため、使用する必要はありません。

---

## 手法3: Chain of Thought (CoT)

**思考プロセスを段階的に外部化させる手法**

LLMには「考える時間」がないため、思考プロセスを出力として表現させることで、より深い推論を促す

<div class="engineer-focus">
💻 **CoTの効果**<br>
・複雑な問題を小さなステップに分解<br>
・中間推論の明示により精度向上<br>
・デバッグしやすい思考プロセス
</div>

```yaml
## タスク
以下のアルゴリズムの時間計算量を分析してください

## 分析手順
1. まず、アルゴリズムの各ステップを特定する
2. 各ステップの実行回数を計算する
3. 最も時間のかかる操作を特定する
4. 全体の時間計算量を導出する
5. 改善可能な点があれば提案する

各ステップで思考プロセスを明示してください。

[コードをここに]
```

---

# 5. LLMアプリケーション

## AIエージェントの仕組み

---


## 「魔法」の正体

TODO:LLMは結局文字列しか受け取らず文字列しか生成しないということを強調する。そのため、Context収集やプロンプト生成を行うアプリケーション層の性能と、その特性を知ることもかなり重要。という旨を強調する。

<div class="highlight">
実際は「コンテキスト収集 + プロンプト生成 + LLM呼び出し」の組み合わせ
</div>

### 代表例
- GitHub Copilot Agent
- Cursor / Windsurf
- Claude / ChatGPT のCode Interpreter
- カスタムGPTs

<div class="engineer-focus">
💻 **重要な認識**: これらは「知能」を持つわけではなく、<br>
「タスク実行をする文字列」を大量学習したLLMが動いているだけ
</div>

## LLMアプリケーションの実態

### GitHub Copilot AgentやJunieも結局は...

<div class="highlight">
🔧 コンテキストを収集 → 文字列を生成 → LLMに送信<br>
📝 "タスク実行をしているという文字列" を大量学習したLLMが動いているだけ
</div>

```mermaid
VSCode → Copilot Agent → Context収集 → LLM → 応答 → ユーザー
```

---

## Copilot Agentの内部処理（推測）

```python
def copilot_agent(user_request, workspace):
    context = {
        "current_file": get_active_file(),
        "related_files": find_related_files(),
        "git_history": get_recent_commits(),
        "project_structure": analyze_workspace(),
        "dependencies": parse_package_files(),
        "user_intent": classify_request(user_request)
    }

    prompt = build_structured_prompt(context, user_request)
    response = llm.generate(prompt)
    return response
```

---

## LLMアプリケーション利用の成功要因（1/2）

### 1. アプリケーションの「コンテキスト収集品質」を理解する

<div class="engineer-focus">
💻 **Copilot の例**<br>
・現在のファイル内容の取得精度<br>
・関連ファイルの発見アルゴリズム<br>
・プロジェクト構造の解析能力<br>
・Git履歴からの文脈推測
</div>

**利用者としてできること**:
- 関連ファイルを適切に配置・命名
- プロジェクト構造を標準的な形に整理
- コメントによる文脈情報の提供

---

## LLMアプリケーション利用の成功要因（2/2）

### 2. アプリケーションの「プロンプト構築方式」に合わせる

**構造化された情報提示を意識**:
- 明確なタスク記述
- 具体的な制約条件
- 期待する出力形式の指定

### 3. LLMの制約を理解した期待値設定

<div class="engineer-focus">
💻 **重要な認識**<br>
・アプリケーションの「理解」も統計的推測にすぎない<br>
・100%の精度は期待できない<br>
・最終的な検証・テストは人間が実施<br>
・段階的なタスク実行が効果的
</div>

---

## LLMアプリケーション利用時の典型的問題（1/2）

### 1. コンテキスト汚染の回避

<div class="problem">
❌ **無関係ファイルの影響**<br>
Copilot が古い実装ファイルや無関係なテストファイルを参照<br>
→ チェーホフの銃の誤謬により誤った提案
</div>

<div class="solution">
✅ **対策**<br>
・`.gitignore` での不要ファイル除外<br>
・古いコードファイルの適切な整理<br>
・明確なディレクトリ構造の維持
</div>

---

## LLMアプリケーション利用時の典型的問題（2/2）

### 2. 指示の具体性不足

<div class="problem">
❌ **曖昧な指示による誤解**<br>
"このバグを修正して" → アプリが想定と異なる修正を実行
</div>

<div class="solution">
✅ **改善例**<br>
"TypeError: 'str' object is not callable エラーを修正。line 23のparse_date()呼び出しが問題。str型変数がメソッドと同名になっている"
</div>

### 3. 過度な期待と依存

<div class="problem">
❌ **「AIが全て解決してくれる」という幻想**<br>
LLMアプリケーションに複雑なタスクを丸投げ
</div>

<div class="solution">
✅ **適切な活用**<br>
・小さなタスクに分解して段階的に実行<br>
・生成結果の検証を怠らない<br>
・人間の判断と組み合わせる
</div>

---

# まとめ

## 成功するLLM活用のポイント

---

## 成功するLLM活用のポイント

### 🧠 **1. LLMは統計的予測エンジン**
**なぜ**: 自己回帰モデル＋トークン化により、文字レベルの厳密処理が苦手
**対策**: 正規表現や厳密な文字列処理は別ツールと組み合わせる

### 🎯 **2. 具体的で構造化された指示**
**なぜ**: 後戻りできない特性により、曖昧な指示では間違った方向に進む
**対策**: 役割・タスク・制約・期待する出力を明確に構造化

### 📝 **3. 段階的なタスク分解**
**なぜ**: 思考時間の欠如により、複雑なタスクを一度に処理できない
**対策**: CoT（思考の連鎖）を活用し、小さなステップに分解

### 🔍 **4. コンテキストの質を重視**
**なぜ**: チェーホフの銃の誤謬により、無関係な情報も重要と解釈する
**対策**: 関連性のある情報のみを厳選し、ノイズを排除

### 🛠 **5. LLMアプリケーションは単なるツール**
**なぜ**: 「知能」はなく、パターン認識による統計的推測にすぎない
**対策**: 過度な期待を避け、検証・テストは人間が責任を持つ
