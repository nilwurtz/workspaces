# LLMアプリケーションへのプロンプトがなぜ失敗するのか のスライドのスクリプト

## 概要
30分くらい。
30スライドくらい。


## この発表のゴールを説明
- LLMを理解することで、これはうまくいきそう感を得る
- LLMを理解することで、なぜ失敗したのかをプロファイルしやすくする

## 本題

### 1. LLMの基本
- **LLMとは。基礎的なことを説明**:
    - 大規模言語モデル（Large Language Model）の略.知能はない。考えてもいない。テキストを補完し続けている。
    - 大量のテキストデータを下に、次の単語を予測するタスクを繰り返すことで、言語のパターンや構造を学習する。
    - 例えば「私は今日学校に行って、友達と」と言ったら、「遊ぶ」や「勉強する」といった単語が続く可能性が高いと判断します。ただそれは、学習データによって異なるので、「対話向け」「小説向け」などで予測されるものは異なる
- **知能はないことを強調。ただ、自然言語の柔軟性によって、応用できる**
    - LLMは人間のような意識や理解を持たない。学習元のデータが人間らしいため、あたかも知能があるように見えるが、実際にはただの統計的な予測を行っている。ただ、単語ベクトルという高次元の空間で、単語同士の関係性を学習している。
    - そういう特性があるので、表現力が高く、単に補完するだけでなく、質問応答やタスクの実行なども可能なベースとなっている。

- **会話はどうやって成り立つのか？**: ChatML形式の文章について説明。こういった形で、AI Agentに代表されるタスク実行型のLLMアプリケーションも作られていることを説明。

### 2. LLMについて深堀り
（この章からは、うまくいくLLM指示、うまくいかないLLM指示を具体的に紹介したあと、それが起こるLLMの特性についてまとめる。具体例については、必ずソフトウェア開発の文脈での話を上げる。うまくいくLLM指示、うまくいかないLLM指示は必ずしもどちらも必要ではない。）

- 自己回帰モデル
 LLMは1回処理するごとに統計的に最も可能性の高い「次のトークン」を予測・生成し、一度出したトークンは後戻りして修正できない。
 なので、文字を逆転したりするのが難しい。「～～。今の文章の文字列は？」より、「これからの文章の文字列を数えてください。~~」のほうが得意。


- LLMは文字を1つずつゆっくり確認できない
LLMは「トークン」と呼ばれる複数文字のチャンクを認識する。最先端のモデルでさえ「strawberryに含まれるRの数は？」といったごく単純な質問にも簡単に惑わされる。単語をひっくり返したり、文字数を数えるのも苦手。
例として、行数を指定してもらうのが苦手とか、AAのような文字列を見るのは難しいとかを紹介

- 後戻り（修正）の不可逆性
LLMは話しながら自分の話したことが間違ったということを消去できない。この特性ゆえに、LLMは筋の通らない方向へと突き進み、結果として「頑固で滑稽に見えてしまう」ことがある。


- 思考時間（内省）の欠如と即時性
LLMは考えているわけではなく、即座に次の単語を予測している。これにより、思考の深さや内省が欠如しているため、複雑な問題解決や長期的な計画には向かない。という説明。


- ハルシネーションと真実バイアス
プロンプトに提示された情報は重要でかつ正しいとみなしがちである。「真実バイアス」。チェーホフの銃の誤謬のことを紹介。

### 3. さらっとプロンプトエンジニアリングに代表的なものに触れる

memo-llm.txtを参照。

### 4. LLMアプリケーション

Copilot Agentに代表されるAIAgentは、結局のところ文字列をコンテキストサイズを満たすようにLLMに渡しているだけである。
これを今回はLLMアプリケーションと呼ぶ。


（ここに図。VSCode内で動くCopilot AgentというLLMアプリケーションが、いい感じにContextを収集して、必要な部分をかき集め、LLMに送信する。）

結局のところ、LLMをうまく使うだけではだめで、LLMアプリケーションがどのようにContextを収集しているかを手触りとして理解する必要がある。
またAIが色々やってくれているように見えるが、「タスク実行をしているという一連の文字列を大量に学習し、パターンを獲得しただけ」のLLMが動いているのである。
あくまで、「エンジニアっぽい動き」の模倣。

そして、先に述べたような特性がある。誤ったものをContextに含めたり、プロンプトが良くない場合、Agentはうまく動かない。